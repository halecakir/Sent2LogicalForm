{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#files\n",
    "TEST_FILE = \"data/test.txt\"\n",
    "TRAIN_FILE = \"data/train.txt\"\n",
    "WHOLE_FILE = \"data/whole.txt\"\n",
    "F_VOCAB_FILE = \"data/vocab.f.txt\"\n",
    "Q_VOCAB_FILE = \"data/vocab.q.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dynet_config\n",
    "# Declare GPU as the default device type\n",
    "dynet_config.set_gpu()\n",
    "# Set some parameters manualy\n",
    "dynet_config.set(mem=400, random_seed=123456789)\n",
    "# Initialize dynet import using above configuration in the current scope\n",
    "import dynet as dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mnnl import RNNSequencePredictor\n",
    "import random\n",
    "import os\n",
    "from utils.io_utils import IOUtils\n",
    "random.seed(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(fh):\n",
    "    for line in fh:\n",
    "        sentence, lf = line.strip().split(\"\\t\")\n",
    "        sentence = sentence.split()\n",
    "        lf = lf.split()\n",
    "        yield sentence, lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_vocab(filename):\n",
    "    t2i = {\"_UNK\": 0, \"<s>\": 1, \"</s>\":2}\n",
    "    with open(filename) as target:\n",
    "        for line in target:\n",
    "            token = line.strip().split()[0]\n",
    "            if token not in t2i:\n",
    "                t2i[token] = len(t2i)\n",
    "    return t2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_equal(gold, predictions):\n",
    "    total_correct = 0.0\n",
    "    if len(gold) == len(predictions):\n",
    "        equal = True\n",
    "        for g, p in zip(gold, predictions):\n",
    "            if g != p:\n",
    "                equal = False\n",
    "        return equal\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq:\n",
    "    def __init__(self, w2i, lf2i, options):\n",
    "        self.options = options\n",
    "        self.w2i = w2i\n",
    "        #self.lf2i = lf2i\n",
    "        #self.i2lf = {lf2i[lf]:lf for lf in lf2i}\n",
    "        self.i2w = {w2i[w]:w for w in w2i}\n",
    "        self.wdims = options.wembedding_dims\n",
    "        #self.lfdims = options.lfembedding_dims\n",
    "        self.ldims = options.lstm_dims\n",
    "        self.ext_embeddings = None\n",
    "        \n",
    "        self.model = dy.ParameterCollection()\n",
    "        self.trainer = dy.AdamTrainer(self.model)\n",
    "        self.wlookup = self.model.add_lookup_parameters((len(w2i), self.wdims))\n",
    "        #self.lflookup = self.model.add_lookup_parameters((len(lf2i), self.lfdims))\n",
    "        self.__load_model()\n",
    "\n",
    "        self.context_encoder = [dy.VanillaLSTMBuilder(1, self.wdims, self.ldims, self.model)]\n",
    "        self.logical_form_decoder = dy.VanillaLSTMBuilder(1, self.wdims , self.ldims, self.model)\n",
    "        \n",
    "        self.W_s = self.model.add_parameters((len(self.w2i), self.ldims))\n",
    "        self.W_sb = self.model.add_parameters((len(self.w2i)))\n",
    "    \n",
    "    def __load_model(self):\n",
    "        if self.options.external_embedding is not None:\n",
    "            if os.path.isfile(os.path.join(self.options.saved_parameters_dir,\n",
    "                                           self.options.saved_prevectors)):\n",
    "                self.__load_external_embeddings(os.path.join(self.options.saved_parameters_dir,\n",
    "                                                             self.options.saved_prevectors),\n",
    "                                                \"pickle\")\n",
    "            else:\n",
    "                self.__load_external_embeddings(self.options.external_embedding,\n",
    "                                                self.options.external_embedding_type)\n",
    "                self.__save_model()\n",
    "    \n",
    "    def __save_model(self):\n",
    "        IOUtils.save_embeddings(os.path.join(self.options.saved_parameters_dir,\n",
    "                                             self.options.saved_prevectors),\n",
    "                                self.ext_embeddings)\n",
    "\n",
    "    def __load_external_embeddings(self, embedding_file, embedding_file_type):\n",
    "        ext_embeddings, ext_emb_dim = IOUtils.load_embeddings_file(\n",
    "            embedding_file,\n",
    "            embedding_file_type,\n",
    "            lower=True)\n",
    "        assert ext_emb_dim == self.wdims\n",
    "        self.ext_embeddings = {}\n",
    "        print(\"Initializing word embeddings by pre-trained vectors\")\n",
    "        count = 0\n",
    "        for word in self.w2i:\n",
    "            if word in ext_embeddings:\n",
    "                count += 1\n",
    "                self.ext_embeddings[word] = ext_embeddings[word]\n",
    "                self.wlookup.init_row(self.w2i[word], ext_embeddings[word])\n",
    "        print(\"Vocab size: %d; #words having pretrained vectors: %d\" % (len(self.w2i), count))\n",
    "    \n",
    "    \n",
    "    def predict(self, test_path, test_num):\n",
    "        with open(test_path, 'r') as test:\n",
    "            total_correct = 0.0\n",
    "            total_examples = 0\n",
    "            for _, (iPair, (sentence, lf)) in zip(range(test_num), enumerate(read_data(test))):\n",
    "                total_examples += 1\n",
    "                dy.renew_cg() \n",
    "                encoder_state = self.context_encoder[0].initial_state()\n",
    "                for entry in sentence:\n",
    "                    encoder_state = encoder_state.add_input(self.wlookup[self.w2i[entry] if entry in self.w2i else self.w2i['_UNK']])                                          \n",
    "                hidden_context = encoder_state.output()\n",
    "                \n",
    "                decoder_state = self.logical_form_decoder.initial_state()\n",
    "                decoder_state.set_h([hidden_context])\n",
    "                predicted_sequence = []\n",
    "                next_input = self.w2i[\"<s>\"]\n",
    "                counter = 0\n",
    "                while True:\n",
    "                    counter += 1\n",
    "                    decoder_state = decoder_state.add_input(self.wlookup[i])\n",
    "                    probs = dy.softmax(self.W_s * decoder_state.output() + self.W_sb)\n",
    "                    next_input = probs.npvalue().argmax()\n",
    "                    if next_input != self.w2i[\"</s>\"] and counter < 50:\n",
    "                        predicted_sequence.append(next_input)\n",
    "                    else:\n",
    "                        break\n",
    "                predictions = [self.i2w[c] for c in predicted_sequence]\n",
    "                if is_equal(lf, predictions):\n",
    "                    total_correct += 1\n",
    "                #print(\"Index {}\\nOriginal : {}\\nPrediction {}\\n\\n\\n\".format(iPair, lf, predictions))\n",
    "            print(\"Accuracy : {}\".format(total_correct/total_examples))\n",
    "                    \n",
    "                    \n",
    "    def train(self, train_path):\n",
    "        total_loss = 0\n",
    "        with open(train_path, 'r') as train:\n",
    "            shuffledData = list(read_data(train))\n",
    "            random.shuffle(shuffledData)\n",
    "            \n",
    "            for iPair, (sentence, lf) in enumerate(shuffledData):\n",
    "                #I-Context Encoding\n",
    "                encoder_state = self.context_encoder[0].initial_state()\n",
    "            \n",
    "                for entry in sentence:\n",
    "                    encoder_state = encoder_state.add_input(self.wlookup[self.w2i[entry] if entry in self.w2i else self.w2i['_UNK']])                                          \n",
    "                hidden_context = encoder_state.output()\n",
    "                \n",
    "                decoder_state = self.logical_form_decoder.initial_state()\n",
    "                decoder_state.set_h([hidden_context])\n",
    "                decoder_in = [self.w2i[\"<s>\"]] + [self.w2i[i] if i in self.w2i else self.w2i['_UNK'] for i in lf]\n",
    "                decoder_out = [self.w2i[i] if i in self.w2i else self.w2i['_UNK'] for i in lf] + [self.w2i[\"</s>\"]]\n",
    "                probs = []\n",
    "                for i in decoder_in:\n",
    "                    decoder_state = decoder_state.add_input(self.wlookup[i])\n",
    "                    p = dy.softmax(self.W_s * decoder_state.output() + self.W_sb)\n",
    "                    probs.append(p)\n",
    "                loss = [-dy.log(dy.pick(p, o)) for p, o in zip(probs, decoder_out)]\n",
    "                loss = dy.esum(loss)\n",
    "                cur_loss = loss.scalar_value()\n",
    "                total_loss += cur_loss\n",
    "                loss.backward()\n",
    "                self.trainer.update()\n",
    "                #if iPair != 0 and iPair % 50 == 0:\n",
    "                #    print(\"Pair:\" + str(iPair) + \" Loss:\" + str(total_loss / (iPair + 1)))\n",
    "                \n",
    "                dy.renew_cg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.wembedding_dims = 300\n",
    "        self.lfembedding_dims = 64\n",
    "        self.lstm_dims = 128\n",
    "        self.external_embedding = \"data/GoogleNews-vectors-negative300-SLIM.bin\"\n",
    "        self.saved_parameters_dir = \"data/saved-parameters\"\n",
    "        self.saved_prevectors = \"GoogleNews-embedings.pickle\"\n",
    "        self.external_embedding_type = \"word2vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2i = read_vocab(Q_VOCAB_FILE)\n",
    "lf2i = read_vocab(F_VOCAB_FILE)\n",
    "for k in lf2i:\n",
    "    if k not in w2i:\n",
    "        w2i[k] = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing word embeddings by pre-trained vectors\n",
      "Vocab size: 163; #words having pretrained vectors: 100\n",
      "Accuracy : 0.0\n",
      "Accuracy : 0.0\n",
      "Accuracy : 0.0\n",
      "Accuracy : 0.0\n",
      "Accuracy : 0.0\n",
      "Accuracy : 0.0\n",
      "Accuracy : 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-183d32a724b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlf2i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWHOLE_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-83eb2b54d741>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_path)\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mcur_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcur_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;31m#if iPair != 0 and iPair % 50 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "options = Options()\n",
    "model = Seq2Seq(w2i, lf2i, options)\n",
    "for i in range(20):\n",
    "    model.train(WHOLE_FILE)\n",
    "    model.predict(TEST_FILE, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
