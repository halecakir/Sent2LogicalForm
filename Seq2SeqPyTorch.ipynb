{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "seed = 10\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_FILE = \"data/test.txt\"\n",
    "TRAIN_FILE = \"data/train.txt\"\n",
    "WHOLE_FILE = \"data/whole.txt\"\n",
    "F_VOCAB_FILE = \"data/vocab.f.txt\"\n",
    "Q_VOCAB_FILE = \"data/vocab.q.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.rnn_size = 50\n",
    "        self.dropout = False\n",
    "        self.init_weight = 0.08\n",
    "        self.decay_rate = 0.985\n",
    "        self.learning_rate = 0.01\n",
    "        self.plot_every = 10\n",
    "        self.print_every = 50\n",
    "        self.grad_clip = 5\n",
    "        self.dropout = 0.2\n",
    "        self.dropoutrec = 0\n",
    "        \n",
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.i2h = nn.Linear(opt.rnn_size, 4 * opt.rnn_size)\n",
    "        self.h2h = nn.Linear(opt.rnn_size, 4 * opt.rnn_size)\n",
    "        if opt.dropoutrec > 0:\n",
    "            self.dropout = nn.Dropout(opt.droputrec)\n",
    "            \n",
    "    def forward(self, x, prev_c, prev_h):\n",
    "        gates = self.i2h(x) + self.h2h(prev_h)\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "        ingate = torch.sigmoid(ingate)\n",
    "        forgetgate = torch.sigmoid(forgetgate)\n",
    "        cellgate = torch.tanh(cellgate)\n",
    "        outgate = torch.sigmoid(outgate)\n",
    "        if self.opt.dropoutrec > 0:\n",
    "            cellgate = self.dropout(cellgate)\n",
    "        cy = (forgetgate * prev_c) + (ingate * cellgate)\n",
    "        hy = outgate * torch.tanh(cy)  # n_b x hidden_dim\n",
    "        return cy, hy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, opt, input_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.hidden_size = opt.rnn_size\n",
    "        self.embedding = nn.Embedding(input_size, self.hidden_size)\n",
    "        self.lstm = LSTM(self.opt)\n",
    "        if opt.dropout > 0:\n",
    "            self.dropout = nn.Dropout(opt.dropout)\n",
    "        self.__initParameters()\n",
    "\n",
    "    def __initParameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                init.uniform_(param, -opt.init_weight, opt.init_weight)\n",
    "                \n",
    "    def forward(self, input_src, prev_c, prev_h):\n",
    "        src_emb = self.embedding(input_src) # batch_size x src_length x emb_size\n",
    "        if self.opt.dropout > 0:\n",
    "            src_emb = self.dropout(src_emb)\n",
    "        prev_cy, prev_hy = self.lstm(src_emb, prev_c, prev_h)\n",
    "        return prev_cy, prev_hy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, opt, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.hidden_size = opt.rnn_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, self.hidden_size)\n",
    "        self.lstm = LSTM(self.opt)\n",
    "        self.linear = nn.Linear(self.hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        if opt.dropout > 0:\n",
    "            self.dropout = nn.Dropout(opt.dropout)\n",
    "        self.__initParameters()\n",
    "\n",
    "    def __initParameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                init.uniform_(param, -opt.init_weight, opt.init_weight)\n",
    "                \n",
    "    def forward(self, input, prev_c, prev_h):\n",
    "        output = self.embedding(input)\n",
    "        if self.opt.dropout > 0:\n",
    "            output = self.dropout(output)\n",
    "        next_c, next_h = self.lstm(output, prev_c, prev_h)\n",
    "        if self.opt.dropout > 0:\n",
    "            next_h = self.dropout(next_h)\n",
    "        h2y = self.linear(next_h)\n",
    "        pred = self.softmax(h2y)\n",
    "        return pred, next_c, next_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(fh):\n",
    "    for line in fh:\n",
    "        sentence, lf = line.strip().split(\"\\t\")\n",
    "        sentence = sentence.split()\n",
    "        lf = lf.split()\n",
    "        yield sentence, lf\n",
    "\n",
    "def read_vocab(filename):\n",
    "    t2i = {\"<s>\": 0, \"</s>\":1, \"UNK\": 2}\n",
    "    with open(filename) as target:\n",
    "        for line in target:\n",
    "            token = line.strip().split()[0]\n",
    "            if token not in t2i:\n",
    "                t2i[token] = len(t2i)\n",
    "    return t2i\n",
    "\n",
    "def is_equal(gold, predictions):\n",
    "    total_correct = 0.0\n",
    "    if len(gold) == len(predictions):\n",
    "        equal = True\n",
    "        for g, p in zip(gold, predictions):\n",
    "            if g != p:\n",
    "                equal = False\n",
    "        return equal\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprare_data(file_name):\n",
    "    shuffledData = None\n",
    "    with open(TRAIN_FILE, 'r') as train:\n",
    "        shuffledData = list(read_data(train))\n",
    "        random.shuffle(shuffledData)\n",
    "    sentence_index_tensors = []\n",
    "    form_index_tensors = []\n",
    "    for sentence in shuffledData:\n",
    "        text_tensor = torch.zeros((1, len(sentence[0]) + 2), dtype=torch.long)\n",
    "        text_tensor[0][0] = w2i[\"<s>\"]\n",
    "        for idx, word in enumerate(sentence[0]):\n",
    "            word_index = w2i[word] if word in w2i else w2i[\"UNK\"]\n",
    "            text_tensor[0][idx+1] = word_index\n",
    "        text_tensor[0][-1] = w2i[\"</s>\"]\n",
    "        sentence_index_tensors.append(text_tensor)\n",
    "        form_tensor = torch.zeros((1, len(sentence[1]) + 2), dtype=torch.long)\n",
    "        form_tensor[0][0] = lf2i[\"<s>\"]\n",
    "        for idx, form in enumerate(sentence[1]):\n",
    "            form_index = lf2i[form] if form in lf2i else lf2i[\"UNK\"]\n",
    "            form_tensor[0][idx+1] = form_index\n",
    "        form_tensor[0][-1] = lf2i[\"</s>\"]\n",
    "        form_index_tensors.append(form_tensor)\n",
    "    return shuffledData, sentence_index_tensors, form_index_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(opt, criterion, encoder_optimizer, decoder_optimizer, encoder, decoder, s1, f1):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    c = torch.zeros((1, opt.rnn_size), dtype=torch.float, requires_grad=True)\n",
    "    h = torch.zeros((1, opt.rnn_size), dtype=torch.float, requires_grad=True)\n",
    "    for i in range(s1.size(1)):\n",
    "        c, h = encoder(s1[:, i], c, h)\n",
    "\n",
    "    #for dec_in in f1:\n",
    "    loss = 0\n",
    "    for i in range(f1.size(1)-1):\n",
    "        pred, c, h = decoder(f1[:, i], c, h)\n",
    "        loss += criterion(pred, f1[:, i+1])\n",
    "    loss.backward()\n",
    "    if opt.grad_clip != -1:\n",
    "        torch.nn.utils.clip_grad_value_(encoder.parameters(),opt.grad_clip)\n",
    "        torch.nn.utils.clip_grad_value_(decoder.parameters(),opt.grad_clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(opt, s1, lf2i, encoder, decoder):\n",
    "    c = torch.zeros((1, opt.rnn_size), dtype=torch.float, requires_grad=True)\n",
    "    h = torch.zeros((1, opt.rnn_size), dtype=torch.float, requires_grad=True)\n",
    "\n",
    "    for i in range(s1.size(1)):\n",
    "        c, h = encoder(s1[:, i], c, h)\n",
    "\n",
    "    prev = torch.tensor([lf2i['<s>']], dtype=torch.long)\n",
    "    predicted_form = []\n",
    "    counter = 0\n",
    "    while True:\n",
    "        counter += 1\n",
    "        pred, c, h = decoder(prev, c, h)\n",
    "        form_id = pred.argmax().item()\n",
    "        prev = torch.tensor([form_id], dtype=torch.long)\n",
    "        if form_id == lf2i[\"</s>\"] or counter >= 100:\n",
    "            break\n",
    "        predicted_form.append(form_id)\n",
    "    return predicted_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "def showPlot(points, fig_name, extra_info):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.title(extra_info) \n",
    "    plt.plot(points)\n",
    "    plt.savefig(\"{}.png\".format(fig_name))\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(epoch_num, directory):\n",
    "    train_data, sentence_index_tensors_train, form_index_tensors_train = preprare_data(TRAIN_FILE)\n",
    "    test_data, sentence_index_tensors_test, form_index_tensors_test = preprare_data(TEST_FILE)\n",
    "    \n",
    "    encoder = Encoder(opt, len(w2i))\n",
    "    decoder = Decoder(opt, len(lf2i))\n",
    "\n",
    "    optim_state = {\"learningRate\" : opt.learning_rate, \"alpha\" :  opt.decay_rate}\n",
    "    encoder_optimizer = optim.RMSprop(encoder.parameters(),  lr=optim_state[\"learningRate\"], alpha=optim_state[\"alpha\"])\n",
    "    decoder_optimizer = optim.RMSprop(decoder.parameters(),  lr=optim_state[\"learningRate\"], alpha=optim_state[\"alpha\"])\n",
    "    criterion = nn.NLLLoss(ignore_index=0)\n",
    "\n",
    "    losses = []\n",
    "    max_acc = 0\n",
    "    maxAccEpochId = 0\n",
    "    accuracies = []\n",
    "    for epoch in range(epoch_num):\n",
    "        print(\"---Epoch {}---\\n\".format(epoch+1))\n",
    "        print(\"Training...\")\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        plot_data = []\n",
    "        for index, (sentence, form) in enumerate(zip(sentence_index_tensors_train, form_index_tensors_train)):\n",
    "            loss = train(opt, criterion, encoder_optimizer, decoder_optimizer, encoder, decoder, sentence, form)\n",
    "            if index != 0:\n",
    "                if index % opt.plot_every == 0:     \n",
    "                    plot_data.append(np.mean(losses[epoch*len(train_data)+index-opt.plot_every:]))\n",
    "                if index % opt.print_every == 0:\n",
    "                    print(\"Index {} Loss {}\".format(index, np.mean(losses[epoch*len(train_data)+index-opt.print_every:])))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        print(\"Predicting..\")\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        correct = 0.0\n",
    "        for index, (sentence, form) in enumerate(zip(sentence_index_tensors_test, form_index_tensors_test)):\n",
    "            prediction = predict(opt, sentence, lf2i, encoder, decoder)\n",
    "            prediction = [i2lf[p] for p in prediction]\n",
    "            #print(test_data[index][1])\n",
    "            #print(prediction)\n",
    "            same = True\n",
    "            for g, p in zip(test_data[index][1], prediction):\n",
    "                if g != p:\n",
    "                    same = False\n",
    "            if same:\n",
    "                correct += 1\n",
    "                #print(\"Correct match \", prediction)\n",
    "        accuracy = 100*(correct/len(test_data))\n",
    "        accuracies.append(accuracy)\n",
    "        if accuracy > max_acc:\n",
    "            max_acc = accuracy\n",
    "            maxAccEpochId = epoch\n",
    "        print(\"Accuracy: \", accuracy)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        file_name = \"{}/epoch.{}\".format(directory, epoch)\n",
    "        extra = \"Mean Loss {0:.2f}\".format(np.mean(losses))\n",
    "        showPlot(plot_data, file_name, extra)\n",
    "    file_name = \"{}/{}\".format(directory, \"accuracies\")\n",
    "    extra = \"Maximum Accuracy {0:.2f} at epoch {1}\".format(np.max(accuracies), maxAccEpochId)\n",
    "    showPlot(accuracies, file_name, extra)\n",
    "    file_name = \"{}/{}\".format(directory, \"all_losses\")\n",
    "    \n",
    "    extra = \"Mean Loss {0:.2f}\".format(np.mean(losses))\n",
    "    showPlot(losses, file_name, extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2i = read_vocab(Q_VOCAB_FILE)\n",
    "lf2i = read_vocab(F_VOCAB_FILE)\n",
    "i2lf = {lf2i[i] : i for i in lf2i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Epoch 1---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 37.062820205688475\n",
      "Index 100 Loss 22.35060528755188\n",
      "Index 150 Loss 20.61336812019348\n",
      "Index 200 Loss 17.10960817337036\n",
      "Index 250 Loss 12.480404286384582\n",
      "Index 300 Loss 15.243009309768677\n",
      "Index 350 Loss 14.816191139221191\n",
      "Index 400 Loss 17.51746823310852\n",
      "Index 450 Loss 15.121167697906493\n",
      "Index 500 Loss 16.21831046581268\n",
      "Index 550 Loss 11.95041048526764\n",
      "Predicting..\n",
      "Accuracy:  0.0\n",
      "---Epoch 2---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 15.567510018348694\n",
      "Index 100 Loss 12.866727714538575\n",
      "Index 150 Loss 14.111239376068115\n",
      "Index 200 Loss 11.657528157234191\n",
      "Index 250 Loss 9.001372170448303\n",
      "Index 300 Loss 11.644767847061157\n",
      "Index 350 Loss 11.676557731628417\n",
      "Index 400 Loss 14.357117009162902\n",
      "Index 450 Loss 12.578595099449158\n",
      "Index 500 Loss 13.046538314819337\n",
      "Index 550 Loss 10.925333523750306\n",
      "Predicting..\n",
      "Accuracy:  5.833333333333333\n",
      "---Epoch 3---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 14.238794927597047\n",
      "Index 100 Loss 11.32036850452423\n",
      "Index 150 Loss 12.073881936073303\n",
      "Index 200 Loss 10.59561942577362\n",
      "Index 250 Loss 8.312183060646056\n",
      "Index 300 Loss 10.286371505260467\n",
      "Index 350 Loss 10.347925307750701\n",
      "Index 400 Loss 13.159063758850097\n",
      "Index 450 Loss 11.67581715106964\n",
      "Index 500 Loss 12.1899902677536\n",
      "Index 550 Loss 9.525344638824462\n",
      "Predicting..\n",
      "Accuracy:  13.166666666666666\n",
      "---Epoch 4---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 12.93131087064743\n",
      "Index 100 Loss 10.345939464569092\n",
      "Index 150 Loss 11.718200507164001\n",
      "Index 200 Loss 9.862428293228149\n",
      "Index 250 Loss 7.805231184959411\n",
      "Index 300 Loss 9.386876468658448\n",
      "Index 350 Loss 9.812666878700256\n",
      "Index 400 Loss 11.93650586605072\n",
      "Index 450 Loss 10.861245741844177\n",
      "Index 500 Loss 11.776226344108581\n",
      "Index 550 Loss 8.732947778701782\n",
      "Predicting..\n",
      "Accuracy:  16.5\n",
      "---Epoch 5---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 11.507067151069641\n",
      "Index 100 Loss 9.343769397735596\n",
      "Index 150 Loss 10.245336990356446\n",
      "Index 200 Loss 8.626230127811432\n",
      "Index 250 Loss 7.201829872131348\n",
      "Index 300 Loss 8.670322370529174\n",
      "Index 350 Loss 9.16921805858612\n",
      "Index 400 Loss 11.346480164527893\n",
      "Index 450 Loss 9.627356700897217\n",
      "Index 500 Loss 9.981398830413818\n",
      "Index 550 Loss 7.687177417278289\n",
      "Predicting..\n",
      "Accuracy:  20.666666666666668\n",
      "---Epoch 6---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 11.675785439014435\n",
      "Index 100 Loss 9.015423555374145\n",
      "Index 150 Loss 9.679211311340332\n",
      "Index 200 Loss 8.038186287879943\n",
      "Index 250 Loss 6.2248130536079405\n",
      "Index 300 Loss 8.58016717195511\n",
      "Index 350 Loss 8.65142852306366\n",
      "Index 400 Loss 10.554059810638428\n",
      "Index 450 Loss 9.217317745685577\n",
      "Index 500 Loss 10.51912019252777\n",
      "Index 550 Loss 7.563358330726624\n",
      "Predicting..\n",
      "Accuracy:  20.666666666666668\n",
      "---Epoch 7---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 10.921630980968475\n",
      "Index 100 Loss 8.226463494300843\n",
      "Index 150 Loss 9.355841846466065\n",
      "Index 200 Loss 8.31680891752243\n",
      "Index 250 Loss 6.044443416595459\n",
      "Index 300 Loss 8.064212584495545\n",
      "Index 350 Loss 7.953947007656097\n",
      "Index 400 Loss 9.96942555308342\n",
      "Index 450 Loss 9.02471197605133\n",
      "Index 500 Loss 9.672659974098206\n",
      "Index 550 Loss 7.324890393018722\n",
      "Predicting..\n",
      "Accuracy:  26.166666666666664\n",
      "---Epoch 8---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 11.380189247131348\n",
      "Index 100 Loss 7.973268277645111\n",
      "Index 150 Loss 9.04620171546936\n",
      "Index 200 Loss 7.515894320011139\n",
      "Index 250 Loss 5.719356546401977\n",
      "Index 300 Loss 7.8462249636650085\n",
      "Index 350 Loss 8.370033338069916\n",
      "Index 400 Loss 10.357592582702637\n",
      "Index 450 Loss 8.569026701450348\n",
      "Index 500 Loss 9.216037003993987\n",
      "Index 550 Loss 6.867166696786881\n",
      "Predicting..\n",
      "Accuracy:  28.833333333333332\n",
      "---Epoch 9---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 10.626662335395814\n",
      "Index 100 Loss 8.662893295288086\n",
      "Index 150 Loss 8.70764731168747\n",
      "Index 200 Loss 7.567396061420441\n",
      "Index 250 Loss 6.4693764877319335\n",
      "Index 300 Loss 7.5045387959480285\n",
      "Index 350 Loss 8.261500923633575\n",
      "Index 400 Loss 9.717153308391572\n",
      "Index 450 Loss 8.739823621511459\n",
      "Index 500 Loss 9.620722506046295\n",
      "Index 550 Loss 7.48702301979065\n",
      "Predicting..\n",
      "Accuracy:  29.666666666666668\n",
      "---Epoch 10---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 10.727330672740937\n",
      "Index 100 Loss 7.587869316339493\n",
      "Index 150 Loss 8.991129319667817\n",
      "Index 200 Loss 6.924880509376526\n",
      "Index 250 Loss 6.147703025341034\n",
      "Index 300 Loss 7.243784153461457\n",
      "Index 350 Loss 7.731830241680146\n",
      "Index 400 Loss 10.014700419902802\n",
      "Index 450 Loss 8.240688116550446\n",
      "Index 500 Loss 9.506266503334045\n",
      "Index 550 Loss 6.9233364844322205\n",
      "Predicting..\n",
      "Accuracy:  26.833333333333332\n",
      "---Epoch 11---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 10.549481296539307\n",
      "Index 100 Loss 7.811863131523133\n",
      "Index 150 Loss 8.56788076877594\n",
      "Index 200 Loss 7.344279074668885\n",
      "Index 250 Loss 5.67103911280632\n",
      "Index 300 Loss 7.302429714202881\n",
      "Index 350 Loss 7.890403311252594\n",
      "Index 400 Loss 9.935556986331939\n",
      "Index 450 Loss 8.782698745727538\n",
      "Index 500 Loss 9.576201422214508\n",
      "Index 550 Loss 7.188957514762879\n",
      "Predicting..\n",
      "Accuracy:  30.0\n",
      "---Epoch 12---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.990070943832398\n",
      "Index 100 Loss 7.676836278438568\n",
      "Index 150 Loss 8.88696438997984\n",
      "Index 200 Loss 7.272564563751221\n",
      "Index 250 Loss 6.13892037153244\n",
      "Index 300 Loss 7.0767385429143905\n",
      "Index 350 Loss 7.366027171611786\n",
      "Index 400 Loss 9.463839583396911\n",
      "Index 450 Loss 7.430198121070862\n",
      "Index 500 Loss 9.46380609512329\n",
      "Index 550 Loss 6.712990013360977\n",
      "Predicting..\n",
      "Accuracy:  30.666666666666664\n",
      "---Epoch 13---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 10.45642776966095\n",
      "Index 100 Loss 7.645459848642349\n",
      "Index 150 Loss 8.97034560918808\n",
      "Index 200 Loss 7.066333218812942\n",
      "Index 250 Loss 5.807089059948921\n",
      "Index 300 Loss 6.9552209448814395\n",
      "Index 350 Loss 7.166952126026153\n",
      "Index 400 Loss 9.297220005989075\n",
      "Index 450 Loss 7.711729289591313\n",
      "Index 500 Loss 8.517739989757537\n",
      "Index 550 Loss 6.801460115909577\n",
      "Predicting..\n",
      "Accuracy:  28.499999999999996\n",
      "---Epoch 14---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 10.645401746034622\n",
      "Index 100 Loss 7.674912334680557\n",
      "Index 150 Loss 8.60338853597641\n",
      "Index 200 Loss 7.053132964372635\n",
      "Index 250 Loss 5.54750972032547\n",
      "Index 300 Loss 6.719662396907807\n",
      "Index 350 Loss 6.733406527042389\n",
      "Index 400 Loss 9.191629465520382\n",
      "Index 450 Loss 7.452123863697052\n",
      "Index 500 Loss 9.032119517326356\n",
      "Index 550 Loss 7.084961364269256\n",
      "Predicting..\n",
      "Accuracy:  31.333333333333336\n",
      "---Epoch 15---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 10.447543287277222\n",
      "Index 100 Loss 7.148166525363922\n",
      "Index 150 Loss 8.17560182929039\n",
      "Index 200 Loss 6.981071499586105\n",
      "Index 250 Loss 5.149356741905212\n",
      "Index 300 Loss 7.461922895908356\n",
      "Index 350 Loss 7.04511246919632\n",
      "Index 400 Loss 9.381061334609985\n",
      "Index 450 Loss 7.722541454434395\n",
      "Index 500 Loss 8.912932400703431\n",
      "Index 550 Loss 6.908009340763092\n",
      "Predicting..\n",
      "Accuracy:  33.166666666666664\n",
      "---Epoch 16---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 10.222803224921227\n",
      "Index 100 Loss 7.145639522075653\n",
      "Index 150 Loss 8.070579199790954\n",
      "Index 200 Loss 7.013913420438766\n",
      "Index 250 Loss 4.443831475377083\n",
      "Index 300 Loss 7.3562052704393865\n",
      "Index 350 Loss 6.867443832159043\n",
      "Index 400 Loss 9.159020357131958\n",
      "Index 450 Loss 7.686902433633804\n",
      "Index 500 Loss 9.337270417213439\n",
      "Index 550 Loss 6.8139367771148684\n",
      "Predicting..\n",
      "Accuracy:  33.166666666666664\n",
      "---Epoch 17---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.111474332809449\n",
      "Index 100 Loss 6.9586494708061215\n",
      "Index 150 Loss 8.162130601406098\n",
      "Index 200 Loss 6.459841732382774\n",
      "Index 250 Loss 5.235021111965179\n",
      "Index 300 Loss 7.330760366916657\n",
      "Index 350 Loss 6.9722813439369205\n",
      "Index 400 Loss 9.093147513866425\n",
      "Index 450 Loss 8.233392314910889\n",
      "Index 500 Loss 8.684786514937878\n",
      "Index 550 Loss 6.650752956569195\n",
      "Predicting..\n",
      "Accuracy:  33.33333333333333\n",
      "---Epoch 18---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.152423884868622\n",
      "Index 100 Loss 7.054640851020813\n",
      "Index 150 Loss 7.969292361736297\n",
      "Index 200 Loss 6.768787252902984\n",
      "Index 250 Loss 5.03120977640152\n",
      "Index 300 Loss 6.734529832601547\n",
      "Index 350 Loss 6.757035455703735\n",
      "Index 400 Loss 8.992713884115219\n",
      "Index 450 Loss 7.629284916520119\n",
      "Index 500 Loss 9.209290068149567\n",
      "Index 550 Loss 6.755583933591843\n",
      "Predicting..\n",
      "Accuracy:  33.0\n",
      "---Epoch 19---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.883940353393555\n",
      "Index 100 Loss 7.239187251329422\n",
      "Index 150 Loss 7.860355306863784\n",
      "Index 200 Loss 6.519712468385697\n",
      "Index 250 Loss 5.263383330106735\n",
      "Index 300 Loss 6.6349772620201115\n",
      "Index 350 Loss 7.278295745830983\n",
      "Index 400 Loss 8.961900004446507\n",
      "Index 450 Loss 8.134390301704407\n",
      "Index 500 Loss 8.781289417743682\n",
      "Index 550 Loss 6.577045767307282\n",
      "Predicting..\n",
      "Accuracy:  34.333333333333336\n",
      "---Epoch 20---\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 50 Loss 9.779156978726387\n",
      "Index 100 Loss 7.051431990861893\n",
      "Index 150 Loss 7.553302385807037\n",
      "Index 200 Loss 6.972201886773109\n",
      "Index 250 Loss 5.146625252962113\n",
      "Index 300 Loss 6.486664817929268\n",
      "Index 350 Loss 7.60764814376831\n",
      "Index 400 Loss 8.862344679832459\n",
      "Index 450 Loss 7.185458706617355\n",
      "Index 500 Loss 8.829319383352995\n",
      "Index 550 Loss 6.588666895031929\n",
      "Predicting..\n",
      "Accuracy:  36.333333333333336\n",
      "---Epoch 21---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.818297124505044\n",
      "Index 100 Loss 6.708420813083649\n",
      "Index 150 Loss 7.440986632108689\n",
      "Index 200 Loss 6.207282903194428\n",
      "Index 250 Loss 4.934462125301361\n",
      "Index 300 Loss 7.186246987581253\n",
      "Index 350 Loss 7.205182533264161\n",
      "Index 400 Loss 8.93486205816269\n",
      "Index 450 Loss 7.479788546562195\n",
      "Index 500 Loss 8.777852116823196\n",
      "Index 550 Loss 6.165952062606811\n",
      "Predicting..\n",
      "Accuracy:  34.0\n",
      "---Epoch 22---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 10.097838764190675\n",
      "Index 100 Loss 6.734761670827866\n",
      "Index 150 Loss 8.053867215514183\n",
      "Index 200 Loss 7.093232629299163\n",
      "Index 250 Loss 4.601878875494004\n",
      "Index 300 Loss 7.126197279691696\n",
      "Index 350 Loss 6.560189171433449\n",
      "Index 400 Loss 8.863301340341568\n",
      "Index 450 Loss 7.620963963270188\n",
      "Index 500 Loss 9.614978738427162\n",
      "Index 550 Loss 6.501373117566109\n",
      "Predicting..\n",
      "Accuracy:  31.5\n",
      "---Epoch 23---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 10.596436984613538\n",
      "Index 100 Loss 6.640921514034272\n",
      "Index 150 Loss 8.228703908920288\n",
      "Index 200 Loss 6.767730444446206\n",
      "Index 250 Loss 5.141850169897079\n",
      "Index 300 Loss 6.383957169055939\n",
      "Index 350 Loss 6.515216079950332\n",
      "Index 400 Loss 8.47449070379138\n",
      "Index 450 Loss 7.696711018085479\n",
      "Index 500 Loss 8.864526936262846\n",
      "Index 550 Loss 6.240447053015232\n",
      "Predicting..\n",
      "Accuracy:  34.333333333333336\n",
      "---Epoch 24---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.79686735033989\n",
      "Index 100 Loss 7.334084177017212\n",
      "Index 150 Loss 8.72543106943369\n",
      "Index 200 Loss 6.619817295074463\n",
      "Index 250 Loss 4.93872992515564\n",
      "Index 300 Loss 7.1472540889680385\n",
      "Index 350 Loss 6.763044275045395\n",
      "Index 400 Loss 8.157315349578857\n",
      "Index 450 Loss 7.618562420606613\n",
      "Index 500 Loss 8.632777038812637\n",
      "Index 550 Loss 6.388466663956642\n",
      "Predicting..\n",
      "Accuracy:  36.333333333333336\n",
      "---Epoch 25---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.363284548521042\n",
      "Index 100 Loss 7.653047949075699\n",
      "Index 150 Loss 8.162361214756965\n",
      "Index 200 Loss 6.634193397164345\n",
      "Index 250 Loss 4.468211169242859\n",
      "Index 300 Loss 6.630808209180832\n",
      "Index 350 Loss 7.349338750839234\n",
      "Index 400 Loss 8.556772341132165\n",
      "Index 450 Loss 7.248817486763\n",
      "Index 500 Loss 8.819186888933181\n",
      "Index 550 Loss 5.917723676860333\n",
      "Predicting..\n",
      "Accuracy:  34.166666666666664\n",
      "---Epoch 26---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 10.100029047131539\n",
      "Index 100 Loss 7.7997579312324525\n",
      "Index 150 Loss 8.191486430168151\n",
      "Index 200 Loss 6.531193332448602\n",
      "Index 250 Loss 5.581738547757268\n",
      "Index 300 Loss 6.222570706605911\n",
      "Index 350 Loss 6.509817245006562\n",
      "Index 400 Loss 8.94618151962757\n",
      "Index 450 Loss 6.9461422461271285\n",
      "Index 500 Loss 8.576980471014977\n",
      "Index 550 Loss 6.205270231962204\n",
      "Predicting..\n",
      "Accuracy:  33.5\n",
      "---Epoch 27---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.558406487703323\n",
      "Index 100 Loss 6.856580475568771\n",
      "Index 150 Loss 8.381048342585563\n",
      "Index 200 Loss 6.538279439210892\n",
      "Index 250 Loss 4.980677869319916\n",
      "Index 300 Loss 6.031288468241692\n",
      "Index 350 Loss 6.182581390738488\n",
      "Index 400 Loss 9.423073929548263\n",
      "Index 450 Loss 7.19083570599556\n",
      "Index 500 Loss 8.835238646268845\n",
      "Index 550 Loss 6.591256329417229\n",
      "Predicting..\n",
      "Accuracy:  32.33333333333333\n",
      "---Epoch 28---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.629519468545913\n",
      "Index 100 Loss 7.2349182999134065\n",
      "Index 150 Loss 8.503518681526185\n",
      "Index 200 Loss 6.917173157334328\n",
      "Index 250 Loss 5.252816796302795\n",
      "Index 300 Loss 7.216984030008316\n",
      "Index 350 Loss 7.360664911270142\n",
      "Index 400 Loss 8.561426237225533\n",
      "Index 450 Loss 8.445714625120162\n",
      "Index 500 Loss 9.328929699659348\n",
      "Index 550 Loss 6.36673015832901\n",
      "Predicting..\n",
      "Accuracy:  29.333333333333332\n",
      "---Epoch 29---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.430937153100967\n",
      "Index 100 Loss 7.163559925556183\n",
      "Index 150 Loss 8.06829839259386\n",
      "Index 200 Loss 6.149333055019379\n",
      "Index 250 Loss 5.08795315861702\n",
      "Index 300 Loss 6.375471076965332\n",
      "Index 350 Loss 6.988439093828202\n",
      "Index 400 Loss 8.031856142878532\n",
      "Index 450 Loss 6.854167466163635\n",
      "Index 500 Loss 8.492729134559632\n",
      "Index 550 Loss 6.48180092394352\n",
      "Predicting..\n",
      "Accuracy:  30.666666666666664\n",
      "---Epoch 30---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.583192884325982\n",
      "Index 100 Loss 7.496431012153625\n",
      "Index 150 Loss 7.2352753084898\n",
      "Index 200 Loss 6.871681427359581\n",
      "Index 250 Loss 5.02949346601963\n",
      "Index 300 Loss 6.518316599428654\n",
      "Index 350 Loss 6.812561075389385\n",
      "Index 400 Loss 9.38626760482788\n",
      "Index 450 Loss 7.436285443902015\n",
      "Index 500 Loss 8.746335854828358\n",
      "Index 550 Loss 6.413207441568375\n",
      "Predicting..\n",
      "Accuracy:  30.666666666666664\n",
      "---Epoch 31---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.25158475637436\n",
      "Index 100 Loss 6.796423465013504\n",
      "Index 150 Loss 7.591715463399887\n",
      "Index 200 Loss 6.318546759486199\n",
      "Index 250 Loss 4.385253630876541\n",
      "Index 300 Loss 6.244223547279835\n",
      "Index 350 Loss 7.131620404720306\n",
      "Index 400 Loss 8.421702183485031\n",
      "Index 450 Loss 6.903927219957113\n",
      "Index 500 Loss 7.91406988799572\n",
      "Index 550 Loss 5.748918925523758\n",
      "Predicting..\n",
      "Accuracy:  35.0\n",
      "---Epoch 32---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.226069139242172\n",
      "Index 100 Loss 6.479263713955879\n",
      "Index 150 Loss 7.807887597084045\n",
      "Index 200 Loss 6.12573340728879\n",
      "Index 250 Loss 5.032994919419289\n",
      "Index 300 Loss 6.336477968990803\n",
      "Index 350 Loss 6.969878219664097\n",
      "Index 400 Loss 8.506724736094474\n",
      "Index 450 Loss 7.507063649445772\n",
      "Index 500 Loss 9.032845846116542\n",
      "Index 550 Loss 6.683686935305595\n",
      "Predicting..\n",
      "Accuracy:  30.166666666666668\n",
      "---Epoch 33---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.988531561493874\n",
      "Index 100 Loss 7.120700087547302\n",
      "Index 150 Loss 7.871919106245041\n",
      "Index 200 Loss 6.151773967444897\n",
      "Index 250 Loss 4.9263730058074\n",
      "Index 300 Loss 6.417414276599884\n",
      "Index 350 Loss 7.367678058147431\n",
      "Index 400 Loss 8.167288976609706\n",
      "Index 450 Loss 7.576430899463594\n",
      "Index 500 Loss 8.784084135293961\n",
      "Index 550 Loss 6.3645212875306605\n",
      "Predicting..\n",
      "Accuracy:  37.333333333333336\n",
      "---Epoch 34---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.683544962406158\n",
      "Index 100 Loss 6.604852877669036\n",
      "Index 150 Loss 8.316493258476257\n",
      "Index 200 Loss 6.733305109143257\n",
      "Index 250 Loss 5.0611696144938465\n",
      "Index 300 Loss 6.556249173879624\n",
      "Index 350 Loss 6.207269568443298\n",
      "Index 400 Loss 9.249745004177093\n",
      "Index 450 Loss 7.037075692415238\n",
      "Index 500 Loss 8.819210455417632\n",
      "Index 550 Loss 6.2400878049433235\n",
      "Predicting..\n",
      "Accuracy:  34.333333333333336\n",
      "---Epoch 35---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.932133712172508\n",
      "Index 100 Loss 6.817642593383789\n",
      "Index 150 Loss 8.104159800857305\n",
      "Index 200 Loss 6.615313575863838\n",
      "Index 250 Loss 4.482048941850662\n",
      "Index 300 Loss 6.661554462909699\n",
      "Index 350 Loss 6.707204740121961\n",
      "Index 400 Loss 9.19966099023819\n",
      "Index 450 Loss 7.308978624343872\n",
      "Index 500 Loss 8.937284929156304\n",
      "Index 550 Loss 5.976814225316048\n",
      "Predicting..\n",
      "Accuracy:  34.66666666666667\n",
      "---Epoch 36---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.90480316311121\n",
      "Index 100 Loss 6.802968961000443\n",
      "Index 150 Loss 8.201648546680808\n",
      "Index 200 Loss 7.14587600260973\n",
      "Index 250 Loss 5.980552318096161\n",
      "Index 300 Loss 6.834345486164093\n",
      "Index 350 Loss 6.657427454590797\n",
      "Index 400 Loss 8.988851422071457\n",
      "Index 450 Loss 7.722582870125771\n",
      "Index 500 Loss 8.740054799765348\n",
      "Index 550 Loss 6.458754816055298\n",
      "Predicting..\n",
      "Accuracy:  31.166666666666664\n",
      "---Epoch 37---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.527917388379574\n",
      "Index 100 Loss 7.597698410153389\n",
      "Index 150 Loss 8.557361212670804\n",
      "Index 200 Loss 6.751228755582124\n",
      "Index 250 Loss 5.107217041254043\n",
      "Index 300 Loss 6.7144619888067245\n",
      "Index 350 Loss 6.910074210166931\n",
      "Index 400 Loss 8.278711624145508\n",
      "Index 450 Loss 8.106305176615715\n",
      "Index 500 Loss 8.217188800573348\n",
      "Index 550 Loss 6.110702915489673\n",
      "Predicting..\n",
      "Accuracy:  35.16666666666667\n",
      "---Epoch 38---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 10.137452660799026\n",
      "Index 100 Loss 7.016061881780624\n",
      "Index 150 Loss 8.022198761701583\n",
      "Index 200 Loss 6.44313162624836\n",
      "Index 250 Loss 4.967099681794643\n",
      "Index 300 Loss 6.507004944682121\n",
      "Index 350 Loss 6.984711815714836\n",
      "Index 400 Loss 8.733265512883664\n",
      "Index 450 Loss 7.199399225115776\n",
      "Index 500 Loss 8.57579665184021\n",
      "Index 550 Loss 6.42397289738059\n",
      "Predicting..\n",
      "Accuracy:  31.0\n",
      "---Epoch 39---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.445034427046776\n",
      "Index 100 Loss 6.769043550491333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 150 Loss 7.646412497758865\n",
      "Index 200 Loss 6.181170696020127\n",
      "Index 250 Loss 5.637913837432861\n",
      "Index 300 Loss 7.367980968356132\n",
      "Index 350 Loss 6.678414969742298\n",
      "Index 400 Loss 9.37062117099762\n",
      "Index 450 Loss 8.104602736234664\n",
      "Index 500 Loss 8.438878471851348\n",
      "Index 550 Loss 5.873487174510956\n",
      "Predicting..\n",
      "Accuracy:  31.333333333333336\n",
      "---Epoch 40---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.947265850752592\n",
      "Index 100 Loss 6.733190488815308\n",
      "Index 150 Loss 7.3545608013868335\n",
      "Index 200 Loss 6.596686439216137\n",
      "Index 250 Loss 4.525652215480805\n",
      "Index 300 Loss 6.944876497983932\n",
      "Index 350 Loss 5.953017948269844\n",
      "Index 400 Loss 8.294635705277324\n",
      "Index 450 Loss 6.76594306319952\n",
      "Index 500 Loss 8.587490468621255\n",
      "Index 550 Loss 6.453695157170296\n",
      "Predicting..\n",
      "Accuracy:  30.333333333333336\n",
      "---Epoch 41---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.624384880065918\n",
      "Index 100 Loss 6.838113038763404\n",
      "Index 150 Loss 7.467932903766632\n",
      "Index 200 Loss 6.18791894055903\n",
      "Index 250 Loss 5.174340655803681\n",
      "Index 300 Loss 7.0265438920259475\n",
      "Index 350 Loss 7.197491949498653\n",
      "Index 400 Loss 7.863231430053711\n",
      "Index 450 Loss 7.411170807182788\n",
      "Index 500 Loss 8.721888855695724\n",
      "Index 550 Loss 6.082470577657222\n",
      "Predicting..\n",
      "Accuracy:  35.333333333333336\n",
      "---Epoch 42---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.689506802558899\n",
      "Index 100 Loss 6.805839373469353\n",
      "Index 150 Loss 7.492708976268768\n",
      "Index 200 Loss 6.233637058734894\n",
      "Index 250 Loss 4.755997482538223\n",
      "Index 300 Loss 6.9231032449007035\n",
      "Index 350 Loss 8.016959679722786\n",
      "Index 400 Loss 8.665568396896123\n",
      "Index 450 Loss 7.375984877347946\n",
      "Index 500 Loss 8.745070420503616\n",
      "Index 550 Loss 6.64618976186961\n",
      "Predicting..\n",
      "Accuracy:  36.16666666666667\n",
      "---Epoch 43---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.045802634209394\n",
      "Index 100 Loss 6.49528240263462\n",
      "Index 150 Loss 8.05667245209217\n",
      "Index 200 Loss 6.1031533873081205\n",
      "Index 250 Loss 5.255476757287979\n",
      "Index 300 Loss 6.237086449861526\n",
      "Index 350 Loss 6.517534152269364\n",
      "Index 400 Loss 8.336670274734496\n",
      "Index 450 Loss 6.770452550649643\n",
      "Index 500 Loss 8.125971157550811\n",
      "Index 550 Loss 6.155196167230606\n",
      "Predicting..\n",
      "Accuracy:  41.16666666666667\n",
      "---Epoch 44---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 10.32803924947977\n",
      "Index 100 Loss 6.790133625566959\n",
      "Index 150 Loss 7.589118574261665\n",
      "Index 200 Loss 6.437363011240959\n",
      "Index 250 Loss 5.189017350673676\n",
      "Index 300 Loss 7.101025367975235\n",
      "Index 350 Loss 6.689704845249653\n",
      "Index 400 Loss 9.076150171756744\n",
      "Index 450 Loss 6.837685981094837\n",
      "Index 500 Loss 8.611546773910522\n",
      "Index 550 Loss 6.522640441060066\n",
      "Predicting..\n",
      "Accuracy:  38.166666666666664\n",
      "---Epoch 45---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.025734745860099\n",
      "Index 100 Loss 6.378576239347458\n",
      "Index 150 Loss 8.40554007306695\n",
      "Index 200 Loss 7.011815335154534\n",
      "Index 250 Loss 4.959666604399681\n",
      "Index 300 Loss 6.749294568300247\n",
      "Index 350 Loss 6.910355126857757\n",
      "Index 400 Loss 8.91291522026062\n",
      "Index 450 Loss 7.629706528186798\n",
      "Index 500 Loss 8.77189255952835\n",
      "Index 550 Loss 6.508404820561409\n",
      "Predicting..\n",
      "Accuracy:  35.333333333333336\n",
      "---Epoch 46---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 8.930909724235535\n",
      "Index 100 Loss 6.476048702001572\n",
      "Index 150 Loss 7.961766580343246\n",
      "Index 200 Loss 6.207791655063629\n",
      "Index 250 Loss 5.262396931648254\n",
      "Index 300 Loss 6.2802738142013546\n",
      "Index 350 Loss 6.415400768518448\n",
      "Index 400 Loss 9.004635127931833\n",
      "Index 450 Loss 6.733959068283439\n",
      "Index 500 Loss 8.704597510099411\n",
      "Index 550 Loss 6.3887806272506715\n",
      "Predicting..\n",
      "Accuracy:  37.0\n",
      "---Epoch 47---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 10.477819162607194\n",
      "Index 100 Loss 6.4122223021090035\n",
      "Index 150 Loss 7.907748715281486\n",
      "Index 200 Loss 6.2936141347885135\n",
      "Index 250 Loss 5.28981261819601\n",
      "Index 300 Loss 5.9593973606824875\n",
      "Index 350 Loss 6.411449027061463\n",
      "Index 400 Loss 8.838016271591187\n",
      "Index 450 Loss 7.197133086323738\n",
      "Index 500 Loss 8.698890260607005\n",
      "Index 550 Loss 6.181074199676513\n",
      "Predicting..\n",
      "Accuracy:  33.5\n",
      "---Epoch 48---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 10.530288548469544\n",
      "Index 100 Loss 7.571482543945312\n",
      "Index 150 Loss 7.270151995122433\n",
      "Index 200 Loss 6.470118038356304\n",
      "Index 250 Loss 4.703711748123169\n",
      "Index 300 Loss 6.5118349838256835\n",
      "Index 350 Loss 6.743878216743469\n",
      "Index 400 Loss 8.366847739815713\n",
      "Index 450 Loss 7.279291476011276\n",
      "Index 500 Loss 7.746888366341591\n",
      "Index 550 Loss 6.173303799778223\n",
      "Predicting..\n",
      "Accuracy:  36.333333333333336\n",
      "---Epoch 49---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.160076702535152\n",
      "Index 100 Loss 7.135848457813263\n",
      "Index 150 Loss 8.039476121664046\n",
      "Index 200 Loss 7.068881739675999\n",
      "Index 250 Loss 4.708246841430664\n",
      "Index 300 Loss 6.8476823604106904\n",
      "Index 350 Loss 7.095102709308267\n",
      "Index 400 Loss 8.92243823647499\n",
      "Index 450 Loss 6.784083754569292\n",
      "Index 500 Loss 7.963768326789141\n",
      "Index 550 Loss 6.562782080173492\n",
      "Predicting..\n",
      "Accuracy:  32.166666666666664\n",
      "---Epoch 50---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 8.546246917843819\n",
      "Index 100 Loss 6.913920767307282\n",
      "Index 150 Loss 7.279181014597416\n",
      "Index 200 Loss 6.610715925693512\n",
      "Index 250 Loss 5.192175168395043\n",
      "Index 300 Loss 7.3469929322600365\n",
      "Index 350 Loss 6.547818549871445\n",
      "Index 400 Loss 7.862327937334776\n",
      "Index 450 Loss 7.577056383490563\n",
      "Index 500 Loss 9.228487255573272\n",
      "Index 550 Loss 5.8716673600673674\n",
      "Predicting..\n",
      "Accuracy:  41.333333333333336\n"
     ]
    }
   ],
   "source": [
    "train_and_test(50, \"out/WithGradientClippingAndDropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Maximum Accuracy 2.79 at epoch 10'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Maximum Accuracy {0:.2f} at epoch {1}\".format(2.7888, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
