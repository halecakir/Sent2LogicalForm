{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "seed = 10\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_FILE = \"data/test.txt\"\n",
    "TRAIN_FILE = \"data/train.txt\"\n",
    "WHOLE_FILE = \"data/whole.txt\"\n",
    "F_VOCAB_FILE = \"data/vocab.f.txt\"\n",
    "Q_VOCAB_FILE = \"data/vocab.q.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.rnn_size = 50\n",
    "        self.dropout = False\n",
    "        self.init_weight = 0.08\n",
    "        self.decay_rate = 0.985\n",
    "        self.learning_rate = 0.01\n",
    "        self.plot_every = 10\n",
    "        self.print_every = 50\n",
    "        \n",
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.i2h = nn.Linear(opt.rnn_size, 4 * opt.rnn_size)\n",
    "        self.h2h = nn.Linear(opt.rnn_size, 4 * opt.rnn_size)\n",
    "        \n",
    "    def forward(self, x, prev_c, prev_h):\n",
    "        gates = self.i2h(x) + self.h2h(prev_h)\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "        ingate = torch.sigmoid(ingate)\n",
    "        forgetgate = torch.sigmoid(forgetgate)\n",
    "        cellgate = torch.tanh(cellgate)\n",
    "        outgate = torch.sigmoid(outgate)\n",
    "        cy = (forgetgate * prev_c) + (ingate * cellgate)\n",
    "        hy = outgate * torch.tanh(cy)  # n_b x hidden_dim\n",
    "        return cy, hy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, opt, input_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.hidden_size = opt.rnn_size\n",
    "        self.embedding = nn.Embedding(input_size, self.hidden_size)\n",
    "        self.lstm = LSTM(self.opt)\n",
    "        self.__initParameters()\n",
    "        \n",
    "    def __initParameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                init.uniform_(param, -opt.init_weight, opt.init_weight)\n",
    "                \n",
    "    def forward(self, input_src, prev_c, prev_h):\n",
    "        src_emb = self.embedding(input_src) # batch_size x src_length x emb_size\n",
    "        prev_cy, prev_hy = self.lstm(src_emb, prev_c, prev_h)\n",
    "        return prev_cy, prev_hy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, opt, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.hidden_size = opt.rnn_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, self.hidden_size)\n",
    "        self.lstm = LSTM(self.opt)\n",
    "        self.linear = nn.Linear(self.hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.__initParameters()\n",
    "\n",
    "    def __initParameters(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                init.uniform_(param, -opt.init_weight, opt.init_weight)\n",
    "                \n",
    "    def forward(self, input, prev_c, prev_h):\n",
    "        output = self.embedding(input)\n",
    "        next_c, next_h = self.lstm(output, prev_c, prev_h)\n",
    "        h2y = self.linear(next_h)\n",
    "        pred = self.softmax(h2y)\n",
    "        return pred, next_c, next_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(fh):\n",
    "    for line in fh:\n",
    "        sentence, lf = line.strip().split(\"\\t\")\n",
    "        sentence = sentence.split()\n",
    "        lf = lf.split()\n",
    "        yield sentence, lf\n",
    "\n",
    "def read_vocab(filename):\n",
    "    t2i = {\"<s>\": 0, \"</s>\":1, \"UNK\": 2}\n",
    "    with open(filename) as target:\n",
    "        for line in target:\n",
    "            token = line.strip().split()[0]\n",
    "            if token not in t2i:\n",
    "                t2i[token] = len(t2i)\n",
    "    return t2i\n",
    "\n",
    "def is_equal(gold, predictions):\n",
    "    total_correct = 0.0\n",
    "    if len(gold) == len(predictions):\n",
    "        equal = True\n",
    "        for g, p in zip(gold, predictions):\n",
    "            if g != p:\n",
    "                equal = False\n",
    "        return equal\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprare_data(file_name):\n",
    "    shuffledData = None\n",
    "    with open(TRAIN_FILE, 'r') as train:\n",
    "        shuffledData = list(read_data(train))\n",
    "        random.shuffle(shuffledData)\n",
    "    sentence_index_tensors = []\n",
    "    form_index_tensors = []\n",
    "    for sentence in shuffledData:\n",
    "        text_tensor = torch.zeros((1, len(sentence[0]) + 2), dtype=torch.long)\n",
    "        text_tensor[0][0] = w2i[\"<s>\"]\n",
    "        for idx, word in enumerate(sentence[0]):\n",
    "            word_index = w2i[word] if word in w2i else w2i[\"UNK\"]\n",
    "            text_tensor[0][idx+1] = word_index\n",
    "        text_tensor[0][-1] = w2i[\"</s>\"]\n",
    "        sentence_index_tensors.append(text_tensor)\n",
    "        form_tensor = torch.zeros((1, len(sentence[1]) + 2), dtype=torch.long)\n",
    "        form_tensor[0][0] = lf2i[\"<s>\"]\n",
    "        for idx, form in enumerate(sentence[1]):\n",
    "            form_index = lf2i[form] if form in lf2i else lf2i[\"UNK\"]\n",
    "            form_tensor[0][idx+1] = form_index\n",
    "        form_tensor[0][-1] = lf2i[\"</s>\"]\n",
    "        form_index_tensors.append(form_tensor)\n",
    "    return shuffledData, sentence_index_tensors, form_index_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(opt, criterion, encoder_optimizer, decoder_optimizer, encoder, decoder, s1, f1):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    c = torch.zeros((1, opt.rnn_size), dtype=torch.float, requires_grad=True)\n",
    "    h = torch.zeros((1, opt.rnn_size), dtype=torch.float, requires_grad=True)\n",
    "    for i in range(s1.size(1)):\n",
    "        c, h = encoder(s1[:, i], c, h)\n",
    "\n",
    "    #for dec_in in f1:\n",
    "    loss = 0\n",
    "    for i in range(f1.size(1)-1):\n",
    "        pred, c, h = decoder(f1[:, i], c, h)\n",
    "        loss += criterion(pred, f1[:, i+1])\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(opt, s1, lf2i, encoder, decoder):\n",
    "    c = torch.zeros((1, opt.rnn_size), dtype=torch.float, requires_grad=True)\n",
    "    h = torch.zeros((1, opt.rnn_size), dtype=torch.float, requires_grad=True)\n",
    "\n",
    "    for i in range(s1.size(1)):\n",
    "        c, h = encoder(s1[:, i], c, h)\n",
    "\n",
    "    prev = torch.tensor([lf2i['<s>']], dtype=torch.long)\n",
    "    predicted_form = []\n",
    "    counter = 0\n",
    "    while True:\n",
    "        counter += 1\n",
    "        pred, c, h = decoder(prev, c, h)\n",
    "        form_id = pred.argmax().item()\n",
    "        prev = torch.tensor([form_id], dtype=torch.long)\n",
    "        if form_id == lf2i[\"</s>\"] or counter >= 100:\n",
    "            break\n",
    "        predicted_form.append(form_id)\n",
    "    return predicted_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "def showPlot(points, fig_name, extra_info):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.title(extra_info) \n",
    "    plt.plot(points)\n",
    "    plt.savefig(\"{}.png\".format(fig_name))\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(epoch_num, directory):\n",
    "    train_data, sentence_index_tensors_train, form_index_tensors_train = preprare_data(TRAIN_FILE)\n",
    "    test_data, sentence_index_tensors_test, form_index_tensors_test = preprare_data(TEST_FILE)\n",
    "    \n",
    "    encoder = Encoder(opt, len(w2i))\n",
    "    decoder = Decoder(opt, len(lf2i))\n",
    "\n",
    "    optim_state = {\"learningRate\" : opt.learning_rate, \"alpha\" :  opt.decay_rate}\n",
    "    encoder_optimizer = optim.RMSprop(encoder.parameters(),  lr=optim_state[\"learningRate\"], alpha=optim_state[\"alpha\"])\n",
    "    decoder_optimizer = optim.RMSprop(decoder.parameters(),  lr=optim_state[\"learningRate\"], alpha=optim_state[\"alpha\"])\n",
    "    criterion = nn.NLLLoss(ignore_index=0)\n",
    "\n",
    "    losses = []\n",
    "    max_acc = 0\n",
    "    maxAccEpochId = 0\n",
    "    accuracies = []\n",
    "    for epoch in range(epoch_num):\n",
    "        print(\"---Epoch {}---\\n\".format(epoch+1))\n",
    "        print(\"Training...\")\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        plot_data = []\n",
    "        for index, (sentence, form) in enumerate(zip(sentence_index_tensors_train, form_index_tensors_train)):\n",
    "            loss = train(opt, criterion, encoder_optimizer, decoder_optimizer, encoder, decoder, sentence, form)\n",
    "            if index != 0:\n",
    "                if index % opt.plot_every == 0:     \n",
    "                    plot_data.append(np.mean(losses[epoch*len(train_data)+index-opt.plot_every:]))\n",
    "                if index % opt.print_every == 0:\n",
    "                    print(\"Index {} Loss {}\".format(index, np.mean(losses[epoch*len(train_data)+index-opt.print_every:])))\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        print(\"Predicting..\")\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        correct = 0.0\n",
    "        for index, (sentence, form) in enumerate(zip(sentence_index_tensors_test, form_index_tensors_test)):\n",
    "            prediction = predict(opt, sentence, lf2i, encoder, decoder)\n",
    "            prediction = [i2lf[p] for p in prediction]\n",
    "            #print(test_data[index][1])\n",
    "            #print(prediction)\n",
    "            same = True\n",
    "            for g, p in zip(test_data[index][1], prediction):\n",
    "                if g != p:\n",
    "                    same = False\n",
    "            if same:\n",
    "                correct += 1\n",
    "                #print(\"Correct match \", prediction)\n",
    "        accuracy = 100*(correct/len(test_data))\n",
    "        accuracies.append(accuracy)\n",
    "        if accuracy > max_acc:\n",
    "            max_acc = accuracy\n",
    "            maxAccEpochId = epoch\n",
    "        print(\"Accuracy: \", accuracy)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        file_name = \"{}/epoch.{}\".format(directory, epoch)\n",
    "        extra = \"Mean Loss {0:.2f}\".format(np.mean(losses))\n",
    "        showPlot(plot_data, file_name, extra)\n",
    "    file_name = \"{}/{}\".format(directory, \"accuracies\")\n",
    "    extra = \"Maximum Accuracy {0:.2f} at epoch {1}\".format(np.max(accuracies), maxAccEpochId)\n",
    "    showPlot(accuracies, file_name, extra)\n",
    "    file_name = \"{}/{}\".format(directory, \"all_losses\")\n",
    "    \n",
    "    extra = \"Mean Loss {0:.2f}\".format(np.mean(losses))\n",
    "    showPlot(losses, file_name, extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2i = read_vocab(Q_VOCAB_FILE)\n",
    "lf2i = read_vocab(F_VOCAB_FILE)\n",
    "i2lf = {lf2i[i] : i for i in lf2i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Epoch 1---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 37.03196257591247\n",
      "Index 100 Loss 19.301327800750734\n",
      "Index 150 Loss 18.62754457950592\n",
      "Index 200 Loss 14.915855112075805\n",
      "Index 250 Loss 10.929440026283265\n",
      "Index 300 Loss 13.538310189247131\n",
      "Index 350 Loss 13.100777382850646\n",
      "Index 400 Loss 15.060738210678101\n",
      "Index 450 Loss 13.375358805656433\n",
      "Index 500 Loss 13.504880437850952\n",
      "Index 550 Loss 10.511771326065064\n",
      "Predicting..\n",
      "Accuracy:  0.0\n",
      "---Epoch 2---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 14.061849336624146\n",
      "Index 100 Loss 10.614443430900574\n",
      "Index 150 Loss 12.085115180015563\n",
      "Index 200 Loss 10.226313848495483\n",
      "Index 250 Loss 7.687451815605163\n",
      "Index 300 Loss 10.063931317329407\n",
      "Index 350 Loss 10.292899270057678\n",
      "Index 400 Loss 11.651476545333862\n",
      "Index 450 Loss 10.357748050689697\n",
      "Index 500 Loss 10.666328377723694\n",
      "Index 550 Loss 8.803197832107545\n",
      "Predicting..\n",
      "Accuracy:  10.166666666666666\n",
      "---Epoch 3---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 11.874512486457824\n",
      "Index 100 Loss 8.644579081535339\n",
      "Index 150 Loss 9.63024573802948\n",
      "Index 200 Loss 8.339484119415284\n",
      "Index 250 Loss 5.879034495353698\n",
      "Index 300 Loss 8.485591855049133\n",
      "Index 350 Loss 8.385227470397949\n",
      "Index 400 Loss 9.410994210243224\n",
      "Index 450 Loss 8.714418935775758\n",
      "Index 500 Loss 9.657257299423218\n",
      "Index 550 Loss 7.158752346038819\n",
      "Predicting..\n",
      "Accuracy:  13.666666666666666\n",
      "---Epoch 4---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 10.468292036056518\n",
      "Index 100 Loss 7.6982683801651\n",
      "Index 150 Loss 8.515418462753296\n",
      "Index 200 Loss 7.255863835811615\n",
      "Index 250 Loss 5.339276125431061\n",
      "Index 300 Loss 7.478065695762634\n",
      "Index 350 Loss 7.177834670543671\n",
      "Index 400 Loss 8.94442111492157\n",
      "Index 450 Loss 8.24663167476654\n",
      "Index 500 Loss 8.93370509147644\n",
      "Index 550 Loss 6.6529884219169615\n",
      "Predicting..\n",
      "Accuracy:  15.333333333333332\n",
      "---Epoch 5---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 9.32945818901062\n",
      "Index 100 Loss 7.40533970117569\n",
      "Index 150 Loss 7.787179815769195\n",
      "Index 200 Loss 6.832953186035156\n",
      "Index 250 Loss 4.9312623023986815\n",
      "Index 300 Loss 6.89060277223587\n",
      "Index 350 Loss 6.5613644862174985\n",
      "Index 400 Loss 8.134775161743164\n",
      "Index 450 Loss 7.568954629898071\n",
      "Index 500 Loss 8.199484982490539\n",
      "Index 550 Loss 6.351309978961945\n",
      "Predicting..\n",
      "Accuracy:  20.666666666666668\n",
      "---Epoch 6---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 8.938101344108581\n",
      "Index 100 Loss 6.720019011497498\n",
      "Index 150 Loss 7.105143473148346\n",
      "Index 200 Loss 6.231825742721558\n",
      "Index 250 Loss 4.4019838190078735\n",
      "Index 300 Loss 6.012048077583313\n",
      "Index 350 Loss 6.357322025299072\n",
      "Index 400 Loss 7.578136817812919\n",
      "Index 450 Loss 7.352782012224197\n",
      "Index 500 Loss 7.738118376731872\n",
      "Index 550 Loss 5.781779384613037\n",
      "Predicting..\n",
      "Accuracy:  26.5\n",
      "---Epoch 7---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 8.360970554351807\n",
      "Index 100 Loss 6.211866216659546\n",
      "Index 150 Loss 6.896120694875717\n",
      "Index 200 Loss 5.774263182878494\n",
      "Index 250 Loss 4.402019741535187\n",
      "Index 300 Loss 5.533464869260788\n",
      "Index 350 Loss 5.857125174999237\n",
      "Index 400 Loss 7.09775417804718\n",
      "Index 450 Loss 6.522407058477402\n",
      "Index 500 Loss 7.5553177642822265\n",
      "Index 550 Loss 5.382043976783752\n",
      "Predicting..\n",
      "Accuracy:  32.83333333333333\n",
      "---Epoch 8---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 7.623035442829132\n",
      "Index 100 Loss 5.838211150169372\n",
      "Index 150 Loss 6.466153486371041\n",
      "Index 200 Loss 5.594426974058151\n",
      "Index 250 Loss 3.8080419266223906\n",
      "Index 300 Loss 5.5725647968053815\n",
      "Index 350 Loss 6.279220381975174\n",
      "Index 400 Loss 6.781880099773407\n",
      "Index 450 Loss 6.144269416332245\n",
      "Index 500 Loss 7.147828483581543\n",
      "Index 550 Loss 5.163438152074814\n",
      "Predicting..\n",
      "Accuracy:  26.333333333333332\n",
      "---Epoch 9---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 7.364983270168304\n",
      "Index 100 Loss 5.546504173278809\n",
      "Index 150 Loss 6.150645486712456\n",
      "Index 200 Loss 5.160424707233906\n",
      "Index 250 Loss 3.613458347320557\n",
      "Index 300 Loss 5.355584833621979\n",
      "Index 350 Loss 5.660006515979767\n",
      "Index 400 Loss 6.111902123689651\n",
      "Index 450 Loss 5.8523187470436095\n",
      "Index 500 Loss 6.9737873089313505\n",
      "Index 550 Loss 5.132624468803406\n",
      "Predicting..\n",
      "Accuracy:  33.166666666666664\n",
      "---Epoch 10---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 7.375960377454757\n",
      "Index 100 Loss 5.226099990606308\n",
      "Index 150 Loss 5.746853349208831\n",
      "Index 200 Loss 5.369244245290756\n",
      "Index 250 Loss 3.5257861614227295\n",
      "Index 300 Loss 5.175185873508453\n",
      "Index 350 Loss 5.482236742973328\n",
      "Index 400 Loss 6.157293398380279\n",
      "Index 450 Loss 5.659891295433044\n",
      "Index 500 Loss 6.181405589580536\n",
      "Index 550 Loss 4.683791377544403\n",
      "Predicting..\n",
      "Accuracy:  39.83333333333333\n",
      "---Epoch 11---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 7.063592448234558\n",
      "Index 100 Loss 4.977564293146133\n",
      "Index 150 Loss 5.69322740316391\n",
      "Index 200 Loss 4.420750486850738\n",
      "Index 250 Loss 2.930611000061035\n",
      "Index 300 Loss 4.328058421611786\n",
      "Index 350 Loss 4.809332442879676\n",
      "Index 400 Loss 5.719315758943558\n",
      "Index 450 Loss 5.214425783157349\n",
      "Index 500 Loss 6.571732059717179\n",
      "Index 550 Loss 4.593119686841964\n",
      "Predicting..\n",
      "Accuracy:  36.666666666666664\n",
      "---Epoch 12---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 7.4263193011283875\n",
      "Index 100 Loss 5.000686846971512\n",
      "Index 150 Loss 6.330645985007286\n",
      "Index 200 Loss 4.760400211811065\n",
      "Index 250 Loss 3.2057502937316893\n",
      "Index 300 Loss 4.662974231243133\n",
      "Index 350 Loss 4.572648968696594\n",
      "Index 400 Loss 5.863679132163525\n",
      "Index 450 Loss 5.099782410860062\n",
      "Index 500 Loss 5.922648463249207\n",
      "Index 550 Loss 4.411324850916863\n",
      "Predicting..\n",
      "Accuracy:  40.166666666666664\n",
      "---Epoch 13---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 6.991458517313004\n",
      "Index 100 Loss 4.9796094506978985\n",
      "Index 150 Loss 5.320157497227192\n",
      "Index 200 Loss 4.2080094385147095\n",
      "Index 250 Loss 2.8475137627124787\n",
      "Index 300 Loss 4.24911593079567\n",
      "Index 350 Loss 4.701595697999\n",
      "Index 400 Loss 5.531806590557099\n",
      "Index 450 Loss 4.844322277903557\n",
      "Index 500 Loss 5.53286679148674\n",
      "Index 550 Loss 4.354272766113281\n",
      "Predicting..\n",
      "Accuracy:  43.5\n",
      "---Epoch 14---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 6.7173477709293365\n",
      "Index 100 Loss 4.7385165405273435\n",
      "Index 150 Loss 5.130679962038994\n",
      "Index 200 Loss 4.155917637348175\n",
      "Index 250 Loss 2.6173307499289513\n",
      "Index 300 Loss 4.530827131271362\n",
      "Index 350 Loss 4.3874464392662045\n",
      "Index 400 Loss 5.397336543798446\n",
      "Index 450 Loss 5.147315899133682\n",
      "Index 500 Loss 5.499595606327057\n",
      "Index 550 Loss 4.28799412637949\n",
      "Predicting..\n",
      "Accuracy:  43.0\n",
      "---Epoch 15---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 7.341953825652599\n",
      "Index 100 Loss 4.901077536344528\n",
      "Index 150 Loss 5.743557507097721\n",
      "Index 200 Loss 4.229129583835602\n",
      "Index 250 Loss 2.658957230746746\n",
      "Index 300 Loss 4.625874693989754\n",
      "Index 350 Loss 4.422975778579712\n",
      "Index 400 Loss 6.122022551298142\n",
      "Index 450 Loss 5.120939961671829\n",
      "Index 500 Loss 5.756178188323974\n",
      "Index 550 Loss 4.0450421833992\n",
      "Predicting..\n",
      "Accuracy:  44.333333333333336\n",
      "---Epoch 16---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 7.010197508335113\n",
      "Index 100 Loss 4.832091751098633\n",
      "Index 150 Loss 5.380471404790878\n",
      "Index 200 Loss 4.281845304369926\n",
      "Index 250 Loss 2.2220952707529067\n",
      "Index 300 Loss 4.509284754693508\n",
      "Index 350 Loss 4.3999225509166715\n",
      "Index 400 Loss 5.175053486824035\n",
      "Index 450 Loss 4.667273677289486\n",
      "Index 500 Loss 6.106321752071381\n",
      "Index 550 Loss 3.900819164514542\n",
      "Predicting..\n",
      "Accuracy:  44.0\n",
      "---Epoch 17---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 6.162881935834885\n",
      "Index 100 Loss 4.723920735120774\n",
      "Index 150 Loss 5.096843214333058\n",
      "Index 200 Loss 3.840270305275917\n",
      "Index 250 Loss 2.6064012563228607\n",
      "Index 300 Loss 3.7929244363307952\n",
      "Index 350 Loss 4.122035422325134\n",
      "Index 400 Loss 5.114665957093239\n",
      "Index 450 Loss 4.533264873027801\n",
      "Index 500 Loss 5.950617097616195\n",
      "Index 550 Loss 3.9374463403224946\n",
      "Predicting..\n",
      "Accuracy:  48.5\n",
      "---Epoch 18---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 6.203143693208695\n",
      "Index 100 Loss 4.5792956078052525\n",
      "Index 150 Loss 5.042118816375733\n",
      "Index 200 Loss 4.1126362121105196\n",
      "Index 250 Loss 2.4276287180185316\n",
      "Index 300 Loss 3.8592142322659493\n",
      "Index 350 Loss 4.0062268614768985\n",
      "Index 400 Loss 4.8762800931930546\n",
      "Index 450 Loss 4.855509194433689\n",
      "Index 500 Loss 5.754555633068085\n",
      "Index 550 Loss 4.3027104657888415\n",
      "Predicting..\n",
      "Accuracy:  46.833333333333336\n",
      "---Epoch 19---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 6.265985594987869\n",
      "Index 100 Loss 4.137758800387383\n",
      "Index 150 Loss 4.660889679789543\n",
      "Index 200 Loss 4.083842294216156\n",
      "Index 250 Loss 2.379526273608208\n",
      "Index 300 Loss 3.933631658554077\n",
      "Index 350 Loss 3.9207477128505706\n",
      "Index 400 Loss 4.66058159828186\n",
      "Index 450 Loss 4.937460045516491\n",
      "Index 500 Loss 5.231134753227234\n",
      "Index 550 Loss 4.288214811086655\n",
      "Predicting..\n",
      "Accuracy:  50.83333333333333\n",
      "---Epoch 20---\n",
      "\n",
      "Training...\n",
      "Index 50 Loss 6.5219206416606905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 100 Loss 4.973894480466843\n",
      "Index 150 Loss 4.67245913207531\n",
      "Index 200 Loss 3.846572909206152\n",
      "Index 250 Loss 2.1966352313756943\n",
      "Index 300 Loss 4.098828374743461\n",
      "Index 350 Loss 3.7654167509078977\n",
      "Index 400 Loss 4.649455487728119\n",
      "Index 450 Loss 4.346639955043793\n",
      "Index 500 Loss 5.3911291354894635\n",
      "Index 550 Loss 4.0443983185291295\n",
      "Predicting..\n",
      "Accuracy:  48.833333333333336\n"
     ]
    }
   ],
   "source": [
    "train_and_test(20, \"out/BaseExperiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Maximum Accuracy 2.79 at epoch 10'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Maximum Accuracy {0:.2f} at epoch {1}\".format(2.7888, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
